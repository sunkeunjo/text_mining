{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런이 내부에 가지고 있는 예제 데이터인 20뉴스그룹 데이터 세트를 이용해 텍스트 분류를 적용해 보겠습니다. \n",
    "텍스트를 보통 피처 벡터화로 변환을 하면 일반적으로 희소 행렬 형태가 됩니다. 그리고 이러한 희소 행렬에 분류를 효과적으로 잘 처리할 수 있는 알고리즘은 아래와 같습니다.\n",
    "> 로지스틱 회귀, 선형 서포트 벡터 머신, 나이브 베이즈 등 \n",
    "\n",
    "튜토리얼에서 배운 것처럼 먼저 텍스트를 정규화시킨뒤 피처 벡터화를 적용해 보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 데이터를 받아오겠습니다.인터넷에 연결시켜와서 받아오는 구조이며 시간이 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# importing packages \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd \n",
    "\n",
    "news_data=fetch_20newsgroups(subset=\"all\",random_state=156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filenames라는 키값이 있지만 api가 인터넷에서내려받아 로컬 컴퓨터에 저장하는 디렉터리 파일명을 지칭합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도\n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "target 클래스의 이름들 \n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(\"target 클래스의 값과 분포도\\n\",pd.Series(news_data['target']).value_counts().sort_index())\n",
    "print(\"target 클래스의 이름들 \\n\",news_data['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target 클래스의 값은 0부터 19까지 총 20개로 구성되어 있으며 위의 출력 결과 처럼 주어졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data['data'][0]) # 개별 데이터의 모양 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 하고 싶은 것은 내용이 어느 카테고리로 분류되는 가 이기 때문에 내용을 제외하고 나머지 정보들은 제거하도록 하겠습니다.\n",
    "\n",
    "여기서는 subset=\"train\"으로 학습용 데이터만 추출하고 불러올 때 내용만 가지고 올 수 있도록 추출하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기 11314, 테스트 데이터 크기 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train=fetch_20newsgroups(subset=\"train\",remove=(\"headers\",\"footers\",\"quotes\"),random_state=156)\n",
    "\n",
    "X_train=train.data\n",
    "y_train=train.target\n",
    "\n",
    "#subset=test로 test 데이터만 추출 \n",
    "test=fetch_20newsgroups(subset=\"test\",remove=(\"headers\",\"footers\",\"quotes\"),random_state=156)\n",
    "\n",
    "X_test=test.data\n",
    "y_test=test.target\n",
    "\n",
    "print(f\"학습 데이터 크기 {len(X_train)}, 테스트 데이터 크기 {len(X_test)}\") # 리스트 형태 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피처 벡터화 변환과 머신러닝 모델 학습/예측/ 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카운트 기반으로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 텍스트의 shape :  (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# CountVectorization으로 피처 벡터화 변환 수행 \n",
    "cnt_vect=CountVectorizer()\n",
    "cnt_vect.fit(X_train,y_train)\n",
    "X_train_cnt_vect=cnt_vect.transform(X_train)\n",
    "\n",
    "# 학습 데이터로 fit된 CountVectorizer를 이용해 테스트 데이터를 피처 벡터화 변환 수행 \n",
    "X_test_cnt_vect=cnt_vect.transform(X_test)\n",
    "\n",
    "print(\"학습 데이터 텍스트의 shape : \", X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 추출한 결과에ㅔ 로지스틱 회귀를 적용해 뉴스 그룹에 대한 분류를 예측해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression의 예측 정확도는 0.607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic regression을 이용해 학습/예측/평가 \n",
    "lr_clf=LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vect,y_train)\n",
    "pred=lr_clf.predict(X_test_cnt_vect)\n",
    "print(f\"CountVectorized Logistic Regression의 예측 정확도는 {accuracy_score(pred,y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf 기반으로 예측 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression의 예측 정확도는 0.674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect=TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train,y_train)\n",
    "tfidf_xtrain=tfidf_vect.transform(X_train)\n",
    "\n",
    "# test도 바꿔 줍니다\n",
    "tfidf_xtest=tfidf_vect.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf=LogisticRegression()\n",
    "lr_clf.fit(tfidf_xtrain,y_train)\n",
    "prediction=lr_clf.predict(tfidf_xtest)\n",
    "\n",
    "print(f\"TF-IDF Logistic Regression의 예측 정확도는 {accuracy_score(y_test,prediction):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 텍스트가 많고 많은 문서를 가진 텍스트 분석에서 카운트 벡터화 보다는 TF-IDF 벡터화가 더 좋은 예측 결과를 도출해 줍니다. \n",
    "\n",
    "지금은 기본 파라미터를 적용했지만 한 번 더 다양한 파라미터를 적용한 뒤에 결과를 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression의 예측 정확도는 0.692\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect=TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2),max_df=300)\n",
    "tfidf_vect.fit(X_train,y_train)\n",
    "xtrain_tfidf=tfidf_vect.transform(X_train)\n",
    "xtest_tfidf=tfidf_vect.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr=LogisticRegression()\n",
    "lr.fit(xtrain_tfidf,y_train)\n",
    "prediction=lr.predict(xtest_tfidf)\n",
    "\n",
    "print(f\"TF-IDF Logistic Regression의 예측 정확도는 {accuracy_score(y_test,prediction):.3f}\")\n",
    "# 진짜 memory많이 잡아먹으니 무조건 함부로 하지 맙시다;;;;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 설정만 해주는 것으로도 예측 정확도는 올라갑니다.\n",
    "\n",
    "이번에는 GridSearchCV를 이용해 로지스틱 회귀의 하이퍼 파라미터 최적화를 수행해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression best C parameter :  {'C': 10}\n",
      "Tf-idf vectorized logistic regression의 예측정확도는 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf_vect=TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train,y_train)\n",
    "tfidf_xtrain=tfidf_vect.transform(X_train)\n",
    "\n",
    "# test도 바꿔 줍니다\n",
    "tfidf_xtest=tfidf_vect.transform(X_test)\n",
    "\n",
    "# 최적 c 값 도출 튜닝 수행 cv는 폴드 세트로 설정 \n",
    "params={\"C\":[0.01,0.1,1,5,10]}\n",
    "grid_cv_lr=GridSearchCV(LogisticRegression(),param_grid=params,cv=3,scoring=\"accuracy\",verbose=1)\n",
    "grid_cv_lr.fit(tfidf_xtrain,y_train)\n",
    "\n",
    "print(\"Logistic Regression best C parameter : \",grid_cv_lr.best_params_)\n",
    "\n",
    "#최적 c값으로 학습된 cv로 예측 및 정확도 평가 \n",
    "prediction=grid_cv_lr.predict(tfidf_xtest)\n",
    "print(\"Tf-idf vectorized logistic regression의 예측정확도는 {0:.3f}\".format(accuracy_score(y_test,prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 만약 ngram이 있는 상태로 했다면 성능이 더 올라갔을 것이라고 기대가 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런 파이프라인(pipeline) 사용 및 GridSearchCV와의 결합 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 Pipeline 클래스를 이용하면 피처 벡터화와 ML 알고리즘 학습/예측을 위한 코드 작성을 한번에 진행할 수 있습니다. 마치 수도관에서 물이 흐르듯 전처리와 모델에 대한 처리를 할 수 있습니다. 이는 텍스트 기반의 피처 벡터화뿐만 아니라 모든 데이터 전처리 작업과 Estimator를 결함할 수 있기에 스케일링 또는 정규화,PCA 등의 변환 작업과 분류,회귀 등의 Estimator를 한 번에 결합할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunkeun_jo\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline을 통한 Logistic Regression의 예측 정확도는 0.701\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#tfidfVectorizer를 tfidf_vector로 LOGISTICREGRESSION객체를 lr_clf로 생성하는 pipeline 생성\n",
    "pipeline=Pipeline([(\"tfidf_vect\",TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2),max_df=300)),(\"lr_clf\",LogisticRegression(C=10))])\n",
    "# 별도의 fit(),transform()이 필요 없음 \n",
    "pipeline.fit(X_train,y_train)\n",
    "pred=pipeline.predict(X_test)\n",
    "\n",
    "print(\"Pipeline을 통한 Logistic Regression의 예측 정확도는 {:.3f}\".format(accuracy_score(pred,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline역시 Grid searchcv와도 같이 쓸 수 있지만 이 때 주의할 점은 모두의 파라미터를 최적화하려면 너무 많은 튜닝시간이 소모됨과 동시에 추천드리지는 않습니다. 해당 코드에서 쓰이는 형태는 추후 필요할 경우 \"파이썬 머신러닝 완벽가이드 p.490\"을 확인해보면 좋습니다.(정확도도 크게 개선되지 않습니다) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추후 다시 하면 좋은 것 \n",
    "* 나이브 베이즈를 이용한 분류 \n",
    "* 서포트 벡터 머신을 이용한 분류"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
